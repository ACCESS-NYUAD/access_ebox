'''
Copyright (C) 2022 Francesco Paparella, Pedro Velasquez

This file is part of "ACCESS IOT Stations".

"ACCESS IOT Stations" is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by the Free
Software Foundation, either version 3 of the License, or (at your option) any
later version.

"ACCESS IOT Stations" is distributed in the hope that it will be useful, but
WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along with
"ACCESS IOT Stations". If not, see <https://www.gnu.org/licenses/>.

'''

import numpy as np
import pymongo
import pandas as pd
import xarray as xr
from datetime import datetime

"""
Running this file will create a copy of a csv and netcdf file of every Station's data in the current 
directory for each month it has been uploading data (excluding station 0). It uses a trimmed version
of the DataStore class emplyed in the iot_data_dashboard.ipynb file responsible for maintaing the 
dashboard.
"""

# constant definitions

MONGO_IP = 'localhost'
MONGO_PORT = 27017
DATABASE = 'stations'

DIRECTORY = '.'

# connect to MongoDB
client = pymongo.MongoClient(host=MONGO_IP, port=MONGO_PORT)
db = client[DATABASE]


class DataStore:
    """
    This class queries the database and holds a pandas dataframe which stores all of the queried data
    It is also capable of outputting the data as a .csv or .nc file.
    """
    def __init__(self, db: pymongo.database.Database, station_num: int = 1):

        self.db = db
        self.station_num = station_num

        # sets self.station to the appriopriate collection in the database
        self.station = db[f'station{station_num}']
        
        
        all_measurements = self.get_all_measurements()
        self.data = self.query(all_measurements)

    def get_all_measurements(self):
        """
        Queries the DB for graphable measurements and returns a list of strings in form sensor.measurement.index, i.e. particulate_matter.PM1count.1
        Excludes the gps data, and sensor, index, and type fields
        """
        measurements = []
        
        # chooses a non-config document
        doc = self.station.find_one({'config' : {"$exists" : False}})

        # iterates through each sensor in the document
        for sensor in doc:

            # excludes the object id field, datetime field, and gps sensor
            if sensor not in ['_id', 'datetime', 'gps']:
                sensor_fields = doc[sensor]

                # iterates through each field in the sensor, excluding the sensor name, index, and type/brand
                for field in sensor_fields:
                    if field not in ['sensor', 'index', 'type']:

                        # adds each field to a list in form [particulate_matter.PM1count.0, air_sensor.humidity.1]
                        measurement = f"{sensor_fields['sensor']}.{field}.{sensor_fields['index']}" 
                        measurements.append(measurement)

        return measurements 

    def query(self, measurements: list):
        """
        Runs an aggregation pipeline to query the database for the data given,
        then loads that data into a pandas dataframe
        
        measurements: a list of measurements in the form sensor.measurement.index. 
        can be generated by get_all_measurements()
        """
        
        # includes only files which have a datetime field
        exclude_config = {'$match': {
            'datetime': {'$exists': True}
            }
        }

        # sorts all documents by the datetime value, from earliest to latest
        sort_by_datetime = {'$sort': {
                'datetime': 1
            }
        }

        # unpacks the fields from the database format, i.e. 
        # {
        # datetime: val
        # particulate_matter+0: {
        #     PM1count: val,
        #     PM1mass: val,
        #     }
        # air_sensor+1: {
        #     humidity: val,
        #     temperature: val
        #     }
        # }
        # becomes:
        # {
        # datetime: val
        # PM1count+0: val
        # PM1mass+0: val
        # humidity+1: val
        # temperature+1: val
        # }
        unpack = {'$project': {
                '_id': 0, 
                'datetime': 1
            }
        }

        # adds each measurement in the measurements parameter to the unpack stage
        for measurement in measurements:
            sensor, field, index = measurement.split(".")
            unpack['$project'][f'{field}+{index}'] = f'${sensor}+{index}.{field}'

        #runs the aggregation
        aggr = self.station.aggregate([exclude_config, sort_by_datetime, unpack], allowDiskUse = True)
        
        #casts the aggregation as a list of dictionaries and then loads that list as a pandas dataframe
        df = pd.DataFrame(list(aggr))

        return df 

    def get_series(self, key: str):
        """
        Returns the given series based on the name of its column. Identical to DataStore[column] or DataStore.data[column]
        """
        return self.data[key]

    def to_csv(self, filename: str, start_date: str = None, end_date: str = None, cols: list = None):   
        """
        Saves the dataframe as a .csv file
        
        filename: The name of the file.
        start_date (Optional): The first day of values to include. Defaults to the first date in the dataframe
        end_date (Optional): The last day of values to include. Defaults to the last date in the dataframe
        cols (Optional): A list of columns to include in the file. Defaults to all columns.
        
        to_csv(filename='january_data',start_date='2023-01',end_date='2023-01',cols=['tempeature+0',temperature+1'])
        will save january_data.csv, containing the temperature values from January 2023
        """
        
        if start_date == None:
            start_date = self['datetime'].iloc[0]
        if end_date == None:
            end_date = self['datetime'].iloc[-1]
            
        if cols == None:
            return self.data.set_index('datetime').loc[start_date:end_date,:].to_csv(f"{filename}", index=True, header=True)
        
        return self.data.set_index('datetime').loc[start_date:end_date,cols].to_csv(f"{filename}", index=True, header=True)
    
    def to_netcdf(self, filename: str, start_date: str = None, end_date: str = None, cols: list = None):
        """
        Saves the dataframe as a .nc file
        
        filename: The name of the file.
        start_date (Optional): The first day of values to include. Defaults to the first date in the dataframe
        end_date (Optional): The last day of values to include. Defaults to the last date in the dataframe
        cols (Optional): A list of columns to include in the file. Defaults to all columns.
        
        to_nc(filename='january_data', start_date='2023-01', end_date='2023-01', cols=['tempeature+0',temperature+1'])
        will save january_data.nc, containing the temperature values from January 2023
        """
        
        if start_date == None:
            start_date = self['datetime'].iloc[0]
        if end_date == None:
            end_date = self['datetime'].iloc[-1]
            
        if cols == None:
            x = xr.Dataset.from_dataframe(self.data.set_index('datetime').loc[start_date:end_date,:])
        else:
            x = xr.Dataset.from_dataframe(self.data.set_index('datetime').loc[start_date:end_date,cols])
            
        ### Per datum in the column, attributes need to be assigned, tentative list includes: full name, unit, sensor of origin, and various sensor specs
            
        return x.to_netcdf(f"{filename}")
    
        
    def __getitem__(self, key: str):
        return self.data[key]

def generate_netcdfs(data_store):

    station = f"Station{data_store.conf['station_num']}"

    for m in data_store.data.set_index('datetime').index.to_period('m').unique():
        
        m = str(m)
        
        filename = f"{station}_{m}.nc"

        data_store.to_netcdf(filename, m,m, None)
    
    return 

def generate_csvs(data_store):

    station = f"Station{data_store.conf['station_num']}"

    for m in data_store.data.set_index('datetime').index.to_period('m').unique():
        
        m = str(m)
        print(m)
        
        filename = f"{station}_{m}.csv"

        data_store.to_csv(filename, m,m, None)
    
    return 

def main(): 
    for doc in db['stations_info'].find():
        if doc['station_num'] != 0:
            data = DataStore(db,doc['station_num'])

            generate_netcdfs(data)
            generate_csvs(data)

if __name__ == "__main__":
    main()